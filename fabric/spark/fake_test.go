// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See LICENSE in the project root for license information.
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.
// SPDX-License-Identifier: MIT

package spark_test

import (
	"context"
	"net/http"
	"testing"

	"github.com/Azure/azure-sdk-for-go/sdk/azcore"

	azfake "github.com/Azure/azure-sdk-for-go/sdk/azcore/fake"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/runtime"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"

	"reflect"
	"time"

	"github.com/stretchr/testify/suite"

	"github.com/microsoft/fabric-sdk-go/fabric/spark"
	"github.com/microsoft/fabric-sdk-go/fabric/spark/fake"
)

var err error

type FakeTestSuite struct {
	suite.Suite

	ctx  context.Context
	cred azcore.TokenCredential

	serverFactory *fake.ServerFactory
	clientFactory *spark.ClientFactory
}

func (testsuite *FakeTestSuite) SetupSuite() {
	testsuite.ctx = context.Background()
	testsuite.cred = &azfake.TokenCredential{}

	testsuite.serverFactory = &fake.ServerFactory{}
	testsuite.clientFactory, err = spark.NewClientFactory(testsuite.cred, nil, &azcore.ClientOptions{
		Transport: fake.NewServerFactoryTransport(testsuite.serverFactory),
	})
	testsuite.Require().NoError(err, "Failed to create client factory")
}

func TestFakeTest(t *testing.T) {
	suite.Run(t, new(FakeTestSuite))
}

func (testsuite *FakeTestSuite) TestWorkspaceSettings_GetSparkSettings() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"Get workspace Spark settings example"},
	})
	var exampleWorkspaceID string
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"

	exampleRes := spark.WorkspaceSparkSettings{
		AutomaticLog: &spark.AutomaticLogProperties{
			Enabled: to.Ptr(true),
		},
		Environment: &spark.EnvironmentProperties{
			Name:           to.Ptr("environment1"),
			RuntimeVersion: to.Ptr("1.2"),
		},
		HighConcurrency: &spark.HighConcurrencyProperties{
			NotebookInteractiveRunEnabled: to.Ptr(true),
			NotebookPipelineRunEnabled:    to.Ptr(false),
		},
		Job: &spark.JobsProperties{
			ConservativeJobAdmissionEnabled: to.Ptr(false),
			SessionTimeoutInMinutes:         to.Ptr[int32](20),
		},
		Pool: &spark.PoolProperties{
			CustomizeComputeEnabled: to.Ptr(true),
			DefaultPool: &spark.InstancePool{
				Name: to.Ptr("Starter Pool"),
				Type: to.Ptr(spark.CustomPoolTypeWorkspace),
				ID:   to.Ptr("00000000-0000-0000-0000-000000000000"),
			},
			StarterPool: &spark.StarterPoolProperties{
				MaxExecutors: to.Ptr[int32](1),
				MaxNodeCount: to.Ptr[int32](2),
			},
		},
	}

	testsuite.serverFactory.WorkspaceSettingsServer.GetSparkSettings = func(ctx context.Context, workspaceID string, options *spark.WorkspaceSettingsClientGetSparkSettingsOptions) (resp azfake.Responder[spark.WorkspaceSettingsClientGetSparkSettingsResponse], errResp azfake.ErrorResponder) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		resp = azfake.Responder[spark.WorkspaceSettingsClientGetSparkSettingsResponse]{}
		resp.SetResponse(http.StatusOK, spark.WorkspaceSettingsClientGetSparkSettingsResponse{WorkspaceSparkSettings: exampleRes}, nil)
		return
	}

	client := testsuite.clientFactory.NewWorkspaceSettingsClient()
	res, err := client.GetSparkSettings(ctx, exampleWorkspaceID, nil)
	testsuite.Require().NoError(err, "Failed to get result for example ")
	testsuite.Require().True(reflect.DeepEqual(exampleRes, res.WorkspaceSparkSettings))
}

func (testsuite *FakeTestSuite) TestWorkspaceSettings_UpdateSparkSettings() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"Update workspace Spark settings default pool using pool ID example"},
	})
	var exampleWorkspaceID string
	var exampleUpdateWorkspaceSettingsRequest spark.UpdateWorkspaceSparkSettingsRequest
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"
	exampleUpdateWorkspaceSettingsRequest = spark.UpdateWorkspaceSparkSettingsRequest{
		Pool: &spark.PoolProperties{
			DefaultPool: &spark.InstancePool{
				ID: to.Ptr("00000000-0000-0000-0000-000000000000"),
			},
		},
	}

	exampleRes := spark.WorkspaceSparkSettings{
		AutomaticLog: &spark.AutomaticLogProperties{
			Enabled: to.Ptr(false),
		},
		Environment: &spark.EnvironmentProperties{
			Name:           to.Ptr("environment1"),
			RuntimeVersion: to.Ptr("1.2"),
		},
		HighConcurrency: &spark.HighConcurrencyProperties{
			NotebookInteractiveRunEnabled: to.Ptr(false),
		},
		Pool: &spark.PoolProperties{
			CustomizeComputeEnabled: to.Ptr(false),
			DefaultPool: &spark.InstancePool{
				Name: to.Ptr("Starter Pool"),
				Type: to.Ptr(spark.CustomPoolTypeWorkspace),
				ID:   to.Ptr("00000000-0000-0000-0000-000000000000"),
			},
			StarterPool: &spark.StarterPoolProperties{
				MaxExecutors: to.Ptr[int32](1),
				MaxNodeCount: to.Ptr[int32](3),
			},
		},
	}

	testsuite.serverFactory.WorkspaceSettingsServer.UpdateSparkSettings = func(ctx context.Context, workspaceID string, updateWorkspaceSettingsRequest spark.UpdateWorkspaceSparkSettingsRequest, options *spark.WorkspaceSettingsClientUpdateSparkSettingsOptions) (resp azfake.Responder[spark.WorkspaceSettingsClientUpdateSparkSettingsResponse], errResp azfake.ErrorResponder) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		testsuite.Require().True(reflect.DeepEqual(exampleUpdateWorkspaceSettingsRequest, updateWorkspaceSettingsRequest))
		resp = azfake.Responder[spark.WorkspaceSettingsClientUpdateSparkSettingsResponse]{}
		resp.SetResponse(http.StatusOK, spark.WorkspaceSettingsClientUpdateSparkSettingsResponse{WorkspaceSparkSettings: exampleRes}, nil)
		return
	}

	client := testsuite.clientFactory.NewWorkspaceSettingsClient()
	res, err := client.UpdateSparkSettings(ctx, exampleWorkspaceID, exampleUpdateWorkspaceSettingsRequest, nil)
	testsuite.Require().NoError(err, "Failed to get result for example ")
	testsuite.Require().True(reflect.DeepEqual(exampleRes, res.WorkspaceSparkSettings))

	// From example
	ctx = runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"Update workspace Spark settings example"},
	})
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"
	exampleUpdateWorkspaceSettingsRequest = spark.UpdateWorkspaceSparkSettingsRequest{
		AutomaticLog: &spark.AutomaticLogProperties{
			Enabled: to.Ptr(false),
		},
		Environment: &spark.EnvironmentProperties{
			Name:           to.Ptr("environment1"),
			RuntimeVersion: to.Ptr("1.2"),
		},
		HighConcurrency: &spark.HighConcurrencyProperties{
			NotebookInteractiveRunEnabled: to.Ptr(false),
			NotebookPipelineRunEnabled:    to.Ptr(false),
		},
		Job: &spark.JobsProperties{
			ConservativeJobAdmissionEnabled: to.Ptr(false),
			SessionTimeoutInMinutes:         to.Ptr[int32](20),
		},
		Pool: &spark.PoolProperties{
			CustomizeComputeEnabled: to.Ptr(false),
			DefaultPool: &spark.InstancePool{
				Name: to.Ptr("Starter Pool"),
				Type: to.Ptr(spark.CustomPoolTypeWorkspace),
			},
			StarterPool: &spark.StarterPoolProperties{
				MaxExecutors: to.Ptr[int32](1),
				MaxNodeCount: to.Ptr[int32](3),
			},
		},
	}

	exampleRes = spark.WorkspaceSparkSettings{
		AutomaticLog: &spark.AutomaticLogProperties{
			Enabled: to.Ptr(false),
		},
		Environment: &spark.EnvironmentProperties{
			Name:           to.Ptr("environment1"),
			RuntimeVersion: to.Ptr("1.2"),
		},
		HighConcurrency: &spark.HighConcurrencyProperties{
			NotebookInteractiveRunEnabled: to.Ptr(false),
			NotebookPipelineRunEnabled:    to.Ptr(false),
		},
		Job: &spark.JobsProperties{
			ConservativeJobAdmissionEnabled: to.Ptr(false),
			SessionTimeoutInMinutes:         to.Ptr[int32](20),
		},
		Pool: &spark.PoolProperties{
			CustomizeComputeEnabled: to.Ptr(false),
			DefaultPool: &spark.InstancePool{
				Name: to.Ptr("Starter Pool"),
				Type: to.Ptr(spark.CustomPoolTypeWorkspace),
				ID:   to.Ptr("00000000-0000-0000-0000-000000000000"),
			},
			StarterPool: &spark.StarterPoolProperties{
				MaxExecutors: to.Ptr[int32](1),
				MaxNodeCount: to.Ptr[int32](3),
			},
		},
	}

	testsuite.serverFactory.WorkspaceSettingsServer.UpdateSparkSettings = func(ctx context.Context, workspaceID string, updateWorkspaceSettingsRequest spark.UpdateWorkspaceSparkSettingsRequest, options *spark.WorkspaceSettingsClientUpdateSparkSettingsOptions) (resp azfake.Responder[spark.WorkspaceSettingsClientUpdateSparkSettingsResponse], errResp azfake.ErrorResponder) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		testsuite.Require().True(reflect.DeepEqual(exampleUpdateWorkspaceSettingsRequest, updateWorkspaceSettingsRequest))
		resp = azfake.Responder[spark.WorkspaceSettingsClientUpdateSparkSettingsResponse]{}
		resp.SetResponse(http.StatusOK, spark.WorkspaceSettingsClientUpdateSparkSettingsResponse{WorkspaceSparkSettings: exampleRes}, nil)
		return
	}

	res, err = client.UpdateSparkSettings(ctx, exampleWorkspaceID, exampleUpdateWorkspaceSettingsRequest, nil)
	testsuite.Require().NoError(err, "Failed to get result for example ")
	testsuite.Require().True(reflect.DeepEqual(exampleRes, res.WorkspaceSparkSettings))
}

func (testsuite *FakeTestSuite) TestCustomPools_ListWorkspaceCustomPools() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"List custom pools example"},
	})
	var exampleWorkspaceID string
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"

	exampleRes := spark.CustomPools{
		Value: []spark.CustomPool{
			{
				Name: to.Ptr("Starter Pool"),
				Type: to.Ptr(spark.CustomPoolTypeWorkspace),
				AutoScale: &spark.AutoScaleProperties{
					Enabled:      to.Ptr(true),
					MaxNodeCount: to.Ptr[int32](10),
					MinNodeCount: to.Ptr[int32](1),
				},
				DynamicExecutorAllocation: &spark.DynamicExecutorAllocationProperties{
					Enabled:      to.Ptr(true),
					MaxExecutors: to.Ptr[int32](9),
					MinExecutors: to.Ptr[int32](1),
				},
				ID:         to.Ptr("00000000-0000-0000-0000-000000000000"),
				NodeFamily: to.Ptr(spark.NodeFamilyMemoryOptimized),
				NodeSize:   to.Ptr(spark.NodeSizeMedium),
			},
			{
				Name: to.Ptr("pool1"),
				Type: to.Ptr(spark.CustomPoolTypeWorkspace),
				AutoScale: &spark.AutoScaleProperties{
					Enabled:      to.Ptr(true),
					MaxNodeCount: to.Ptr[int32](4),
					MinNodeCount: to.Ptr[int32](1),
				},
				DynamicExecutorAllocation: &spark.DynamicExecutorAllocationProperties{
					Enabled:      to.Ptr(true),
					MaxExecutors: to.Ptr[int32](2),
					MinExecutors: to.Ptr[int32](1),
				},
				ID:         to.Ptr("2367293d-b70b-4b33-97f2-161b8d04a8d7"),
				NodeFamily: to.Ptr(spark.NodeFamilyMemoryOptimized),
				NodeSize:   to.Ptr(spark.NodeSizeSmall),
			}},
	}

	testsuite.serverFactory.CustomPoolsServer.NewListWorkspaceCustomPoolsPager = func(workspaceID string, options *spark.CustomPoolsClientListWorkspaceCustomPoolsOptions) (resp azfake.PagerResponder[spark.CustomPoolsClientListWorkspaceCustomPoolsResponse]) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		resp = azfake.PagerResponder[spark.CustomPoolsClientListWorkspaceCustomPoolsResponse]{}
		resp.AddPage(http.StatusOK, spark.CustomPoolsClientListWorkspaceCustomPoolsResponse{CustomPools: exampleRes}, nil)
		return
	}

	client := testsuite.clientFactory.NewCustomPoolsClient()
	pager := client.NewListWorkspaceCustomPoolsPager(exampleWorkspaceID, &spark.CustomPoolsClientListWorkspaceCustomPoolsOptions{ContinuationToken: nil})
	for pager.More() {
		nextResult, err := pager.NextPage(ctx)
		testsuite.Require().NoError(err, "Failed to advance page for example ")
		testsuite.Require().True(reflect.DeepEqual(exampleRes, nextResult.CustomPools))
		if err == nil {
			break
		}
	}

	// From example
	ctx = runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"List custom pools with continuation example"},
	})
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"

	exampleRes = spark.CustomPools{
		ContinuationToken: to.Ptr("LDEsMTAwMDAwLDA%3D"),
		ContinuationURI:   to.Ptr("https://api.fabric.microsoft.com/v1/workspaces/f089354e-8366-4e18-aea3-4cb4a3a50b48/spark/pools?continuationToken=LDEsMTAwMDAwLDA%3D"),
		Value: []spark.CustomPool{
			{
				Name: to.Ptr("pool1"),
				Type: to.Ptr(spark.CustomPoolTypeWorkspace),
				AutoScale: &spark.AutoScaleProperties{
					Enabled:      to.Ptr(true),
					MaxNodeCount: to.Ptr[int32](4),
					MinNodeCount: to.Ptr[int32](1),
				},
				DynamicExecutorAllocation: &spark.DynamicExecutorAllocationProperties{
					Enabled:      to.Ptr(true),
					MaxExecutors: to.Ptr[int32](2),
					MinExecutors: to.Ptr[int32](1),
				},
				ID:         to.Ptr("2367293d-b70b-4b33-97f2-161b8d04a8d7"),
				NodeFamily: to.Ptr(spark.NodeFamilyMemoryOptimized),
				NodeSize:   to.Ptr(spark.NodeSizeSmall),
			}},
	}

	testsuite.serverFactory.CustomPoolsServer.NewListWorkspaceCustomPoolsPager = func(workspaceID string, options *spark.CustomPoolsClientListWorkspaceCustomPoolsOptions) (resp azfake.PagerResponder[spark.CustomPoolsClientListWorkspaceCustomPoolsResponse]) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		resp = azfake.PagerResponder[spark.CustomPoolsClientListWorkspaceCustomPoolsResponse]{}
		resp.AddPage(http.StatusOK, spark.CustomPoolsClientListWorkspaceCustomPoolsResponse{CustomPools: exampleRes}, nil)
		return
	}

	pager = client.NewListWorkspaceCustomPoolsPager(exampleWorkspaceID, &spark.CustomPoolsClientListWorkspaceCustomPoolsOptions{ContinuationToken: nil})
	for pager.More() {
		nextResult, err := pager.NextPage(ctx)
		testsuite.Require().NoError(err, "Failed to advance page for example ")
		testsuite.Require().True(reflect.DeepEqual(exampleRes, nextResult.CustomPools))
		if err == nil {
			break
		}
	}
}

func (testsuite *FakeTestSuite) TestCustomPools_CreateWorkspaceCustomPool() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"Create custom pool example"},
	})
	var exampleWorkspaceID string
	var exampleCreateCustomPoolRequest spark.CreateCustomPoolRequest
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"
	exampleCreateCustomPoolRequest = spark.CreateCustomPoolRequest{
		Name: to.Ptr("pool1"),
		AutoScale: &spark.AutoScaleProperties{
			Enabled:      to.Ptr(true),
			MaxNodeCount: to.Ptr[int32](2),
			MinNodeCount: to.Ptr[int32](1),
		},
		DynamicExecutorAllocation: &spark.DynamicExecutorAllocationProperties{
			Enabled:      to.Ptr(true),
			MaxExecutors: to.Ptr[int32](1),
			MinExecutors: to.Ptr[int32](1),
		},
		NodeFamily: to.Ptr(spark.NodeFamilyMemoryOptimized),
		NodeSize:   to.Ptr(spark.NodeSizeSmall),
	}

	testsuite.serverFactory.CustomPoolsServer.CreateWorkspaceCustomPool = func(ctx context.Context, workspaceID string, createCustomPoolRequest spark.CreateCustomPoolRequest, options *spark.CustomPoolsClientCreateWorkspaceCustomPoolOptions) (resp azfake.Responder[spark.CustomPoolsClientCreateWorkspaceCustomPoolResponse], errResp azfake.ErrorResponder) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		testsuite.Require().True(reflect.DeepEqual(exampleCreateCustomPoolRequest, createCustomPoolRequest))
		resp = azfake.Responder[spark.CustomPoolsClientCreateWorkspaceCustomPoolResponse]{}
		resp.SetResponse(http.StatusCreated, spark.CustomPoolsClientCreateWorkspaceCustomPoolResponse{}, nil)
		return
	}

	client := testsuite.clientFactory.NewCustomPoolsClient()
	_, err = client.CreateWorkspaceCustomPool(ctx, exampleWorkspaceID, exampleCreateCustomPoolRequest, nil)
	testsuite.Require().NoError(err, "Failed to get result for example ")
}

func (testsuite *FakeTestSuite) TestCustomPools_GetWorkspaceCustomPool() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"Get custom pool example"},
	})
	var exampleWorkspaceID string
	var examplePoolID string
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"
	examplePoolID = "2367293d-b70b-4b33-97f2-161b8d04a8d7"

	exampleRes := spark.CustomPool{
		Name: to.Ptr("pool1"),
		Type: to.Ptr(spark.CustomPoolTypeWorkspace),
		AutoScale: &spark.AutoScaleProperties{
			Enabled:      to.Ptr(true),
			MaxNodeCount: to.Ptr[int32](4),
			MinNodeCount: to.Ptr[int32](1),
		},
		DynamicExecutorAllocation: &spark.DynamicExecutorAllocationProperties{
			Enabled:      to.Ptr(true),
			MaxExecutors: to.Ptr[int32](2),
			MinExecutors: to.Ptr[int32](1),
		},
		ID:         to.Ptr("2367293d-b70b-4b33-97f2-161b8d04a8d7"),
		NodeFamily: to.Ptr(spark.NodeFamilyMemoryOptimized),
		NodeSize:   to.Ptr(spark.NodeSizeSmall),
	}

	testsuite.serverFactory.CustomPoolsServer.GetWorkspaceCustomPool = func(ctx context.Context, workspaceID string, poolID string, options *spark.CustomPoolsClientGetWorkspaceCustomPoolOptions) (resp azfake.Responder[spark.CustomPoolsClientGetWorkspaceCustomPoolResponse], errResp azfake.ErrorResponder) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		testsuite.Require().Equal(examplePoolID, poolID)
		resp = azfake.Responder[spark.CustomPoolsClientGetWorkspaceCustomPoolResponse]{}
		resp.SetResponse(http.StatusOK, spark.CustomPoolsClientGetWorkspaceCustomPoolResponse{CustomPool: exampleRes}, nil)
		return
	}

	client := testsuite.clientFactory.NewCustomPoolsClient()
	res, err := client.GetWorkspaceCustomPool(ctx, exampleWorkspaceID, examplePoolID, nil)
	testsuite.Require().NoError(err, "Failed to get result for example ")
	testsuite.Require().True(reflect.DeepEqual(exampleRes, res.CustomPool))
}

func (testsuite *FakeTestSuite) TestCustomPools_DeleteWorkspaceCustomPool() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"Delete custom pool example"},
	})
	var exampleWorkspaceID string
	var examplePoolID string
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"
	examplePoolID = "2367293d-b70b-4b33-97f2-161b8d04a8d7"

	testsuite.serverFactory.CustomPoolsServer.DeleteWorkspaceCustomPool = func(ctx context.Context, workspaceID string, poolID string, options *spark.CustomPoolsClientDeleteWorkspaceCustomPoolOptions) (resp azfake.Responder[spark.CustomPoolsClientDeleteWorkspaceCustomPoolResponse], errResp azfake.ErrorResponder) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		testsuite.Require().Equal(examplePoolID, poolID)
		resp = azfake.Responder[spark.CustomPoolsClientDeleteWorkspaceCustomPoolResponse]{}
		resp.SetResponse(http.StatusOK, spark.CustomPoolsClientDeleteWorkspaceCustomPoolResponse{}, nil)
		return
	}

	client := testsuite.clientFactory.NewCustomPoolsClient()
	_, err = client.DeleteWorkspaceCustomPool(ctx, exampleWorkspaceID, examplePoolID, nil)
	testsuite.Require().NoError(err, "Failed to get result for example ")
}

func (testsuite *FakeTestSuite) TestCustomPools_UpdateWorkspaceCustomPool() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"Update custom pool example"},
	})
	var exampleWorkspaceID string
	var examplePoolID string
	var exampleUpdateCustomPoolRequest spark.UpdateCustomPoolRequest
	exampleWorkspaceID = "f089354e-8366-4e18-aea3-4cb4a3a50b48"
	examplePoolID = "2367293d-b70b-4b33-97f2-161b8d04a8d7"
	exampleUpdateCustomPoolRequest = spark.UpdateCustomPoolRequest{
		Name: to.Ptr("pool1"),
		AutoScale: &spark.AutoScaleProperties{
			Enabled:      to.Ptr(true),
			MaxNodeCount: to.Ptr[int32](2),
			MinNodeCount: to.Ptr[int32](1),
		},
		DynamicExecutorAllocation: &spark.DynamicExecutorAllocationProperties{
			Enabled:      to.Ptr(true),
			MaxExecutors: to.Ptr[int32](1),
			MinExecutors: to.Ptr[int32](1),
		},
		NodeFamily: to.Ptr(spark.NodeFamilyMemoryOptimized),
		NodeSize:   to.Ptr(spark.NodeSizeSmall),
	}

	exampleRes := spark.CustomPool{
		Name: to.Ptr("pool1"),
		Type: to.Ptr(spark.CustomPoolTypeWorkspace),
		AutoScale: &spark.AutoScaleProperties{
			Enabled:      to.Ptr(true),
			MaxNodeCount: to.Ptr[int32](2),
			MinNodeCount: to.Ptr[int32](1),
		},
		DynamicExecutorAllocation: &spark.DynamicExecutorAllocationProperties{
			Enabled:      to.Ptr(true),
			MaxExecutors: to.Ptr[int32](1),
			MinExecutors: to.Ptr[int32](1),
		},
		ID:         to.Ptr("2367293d-b70b-4b33-97f2-161b8d04a8d7"),
		NodeFamily: to.Ptr(spark.NodeFamilyMemoryOptimized),
		NodeSize:   to.Ptr(spark.NodeSizeSmall),
	}

	testsuite.serverFactory.CustomPoolsServer.UpdateWorkspaceCustomPool = func(ctx context.Context, workspaceID string, poolID string, updateCustomPoolRequest spark.UpdateCustomPoolRequest, options *spark.CustomPoolsClientUpdateWorkspaceCustomPoolOptions) (resp azfake.Responder[spark.CustomPoolsClientUpdateWorkspaceCustomPoolResponse], errResp azfake.ErrorResponder) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		testsuite.Require().Equal(examplePoolID, poolID)
		testsuite.Require().True(reflect.DeepEqual(exampleUpdateCustomPoolRequest, updateCustomPoolRequest))
		resp = azfake.Responder[spark.CustomPoolsClientUpdateWorkspaceCustomPoolResponse]{}
		resp.SetResponse(http.StatusOK, spark.CustomPoolsClientUpdateWorkspaceCustomPoolResponse{CustomPool: exampleRes}, nil)
		return
	}

	client := testsuite.clientFactory.NewCustomPoolsClient()
	res, err := client.UpdateWorkspaceCustomPool(ctx, exampleWorkspaceID, examplePoolID, exampleUpdateCustomPoolRequest, nil)
	testsuite.Require().NoError(err, "Failed to get result for example ")
	testsuite.Require().True(reflect.DeepEqual(exampleRes, res.CustomPool))
}

func (testsuite *FakeTestSuite) TestLivySessions_ListLivySessions() {
	// From example
	ctx := runtime.WithHTTPHeader(testsuite.ctx, map[string][]string{
		"example-id": {"List all livy sessions example"},
	})
	var exampleWorkspaceID string
	exampleWorkspaceID = "f8113ba8-dd81-443e-811a-b385340f3f05"

	exampleRes := spark.LivySessions{
		Value: []spark.LivySession{
			{
				AttemptNumber:      to.Ptr[int32](1),
				CancellationReason: to.Ptr("User cancelled the Spark batch"),
				CapacityID:         to.Ptr("3c0cd366-dc28-4b6d-a525-4d415a8666e7"),
				CreatorItem: &spark.ItemReferenceByID{
					ReferenceType: to.Ptr(spark.ItemReferenceTypeByID),
					ItemID:        to.Ptr("8cee7699-2e81-4121-9a53-cc9025046193"),
					WorkspaceID:   to.Ptr("f8113ba8-dd81-443e-811a-b385340f3f05"),
				},
				EndDateTime: to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:37:30.000Z"); return t }()),
				Item: &spark.ItemReferenceByID{
					ReferenceType: to.Ptr(spark.ItemReferenceTypeByID),
					ItemID:        to.Ptr("8cee7699-2e81-4121-9a53-cc9025046193"),
					WorkspaceID:   to.Ptr("f8113ba8-dd81-443e-811a-b385340f3f05"),
				},
				ItemName:                   to.Ptr("nb_itemName1"),
				ItemType:                   to.Ptr(spark.ItemTypeNotebook),
				JobInstanceID:              to.Ptr("c2baabbd-5327-430c-87a6-ff4f98285601"),
				JobType:                    to.Ptr(spark.JobTypeSparkBatch),
				LivyID:                     to.Ptr("9611f500-bf44-42e0-a0de-78dacb374398"),
				LivyName:                   to.Ptr("random_test_name_app"),
				LivySessionItemResourceURI: to.Ptr(""),
				MaxNumberOfAttempts:        to.Ptr[int32](1),
				OperationName:              to.Ptr("Batch Livy Run"),
				Origin:                     to.Ptr(spark.OriginSubmittedJob),
				QueuedDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](1),
				},
				RunningDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](180),
				},
				RuntimeVersion:     to.Ptr("1.3"),
				SparkApplicationID: to.Ptr("application_1730933685452_0001"),
				StartDateTime:      to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:34:11.000Z"); return t }()),
				State:              to.Ptr(spark.StateCancelled),
				SubmittedDateTime:  to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:32:03.000Z"); return t }()),
				Submitter: &spark.Principal{
					Type: to.Ptr(spark.PrincipalTypeUser),
					ID:   to.Ptr("6f23a8a6-d954-4550-b91a-4df73ccd0311"),
				},
				TotalDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](360),
				},
			},
			{
				AttemptNumber:      to.Ptr[int32](1),
				CancellationReason: to.Ptr("User cancelled the Spark batch"),
				CapacityID:         to.Ptr("3c0cd366-dc28-4b6d-a525-4d415a8666e7"),
				CreatorItem: &spark.ItemReferenceByID{
					ReferenceType: to.Ptr(spark.ItemReferenceTypeByID),
					ItemID:        to.Ptr("7dee7699-2e81-4121-9a53-cc9025046197"),
					WorkspaceID:   to.Ptr("f8113ba8-dd81-443e-811a-b385340f3f05"),
				},
				EndDateTime: to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:37:30.000Z"); return t }()),
				Item: &spark.ItemReferenceByID{
					ReferenceType: to.Ptr(spark.ItemReferenceTypeByID),
					ItemID:        to.Ptr("7dee7699-2e81-4121-9a53-cc9025046197"),
					WorkspaceID:   to.Ptr("f8113ba8-dd81-443e-811a-b385340f3f05"),
				},
				ItemName:                   to.Ptr("lh_itemName2"),
				ItemType:                   to.Ptr(spark.ItemTypeLakehouse),
				JobInstanceID:              to.Ptr("c2baabbd-5327-430c-87a6-ff4f98285601"),
				JobType:                    to.Ptr(spark.JobTypeSparkBatch),
				LivyID:                     to.Ptr("4311f500-bf44-42e0-a0de-78dacb374397"),
				LivyName:                   to.Ptr("random_test_name_app"),
				LivySessionItemResourceURI: to.Ptr(""),
				MaxNumberOfAttempts:        to.Ptr[int32](1),
				OperationName:              to.Ptr("Batch Livy Run"),
				Origin:                     to.Ptr(spark.OriginSubmittedJob),
				QueuedDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](1),
				},
				RunningDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](180),
				},
				RuntimeVersion:     to.Ptr("1.3"),
				SparkApplicationID: to.Ptr("application_1730933685452_0001"),
				StartDateTime:      to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:34:11.000Z"); return t }()),
				State:              to.Ptr(spark.StateCancelled),
				SubmittedDateTime:  to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:32:03.000Z"); return t }()),
				Submitter: &spark.Principal{
					Type: to.Ptr(spark.PrincipalTypeUser),
					ID:   to.Ptr("6f23a8a6-d954-4550-b91a-4df73ccd0311"),
				},
				TotalDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](360),
				},
			},
			{
				AttemptNumber:      to.Ptr[int32](1),
				CancellationReason: to.Ptr("User cancelled the Spark batch"),
				CapacityID:         to.Ptr("3c0cd366-dc28-4b6d-a525-4d415a8666e7"),
				CreatorItem: &spark.ItemReferenceByID{
					ReferenceType: to.Ptr(spark.ItemReferenceTypeByID),
					ItemID:        to.Ptr("4aee7698-2e81-4121-9a53-cc9025046198"),
					WorkspaceID:   to.Ptr("f8113ba8-dd81-443e-811a-b385340f3f05"),
				},
				EndDateTime: to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:37:30.000Z"); return t }()),
				Item: &spark.ItemReferenceByID{
					ReferenceType: to.Ptr(spark.ItemReferenceTypeByID),
					ItemID:        to.Ptr("4aee7698-2e81-4121-9a53-cc9025046198"),
					WorkspaceID:   to.Ptr("f8113ba8-dd81-443e-811a-b385340f3f05"),
				},
				ItemName:                   to.Ptr("sjd_itemName3"),
				ItemType:                   to.Ptr(spark.ItemTypeSparkJobDefinition),
				JobInstanceID:              to.Ptr("c2baabbd-5327-430c-87a6-ff4f98285601"),
				JobType:                    to.Ptr(spark.JobTypeSparkBatch),
				LivyID:                     to.Ptr("7611f500-bf44-42e0-a0de-78dacb374395"),
				LivyName:                   to.Ptr("random_test_name_app"),
				LivySessionItemResourceURI: to.Ptr(""),
				MaxNumberOfAttempts:        to.Ptr[int32](1),
				OperationName:              to.Ptr("Batch Livy Run"),
				Origin:                     to.Ptr(spark.OriginSubmittedJob),
				QueuedDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](1),
				},
				RunningDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](180),
				},
				RuntimeVersion:     to.Ptr("1.3"),
				SparkApplicationID: to.Ptr("application_1730933685452_0001"),
				StartDateTime:      to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:34:11.000Z"); return t }()),
				State:              to.Ptr(spark.StateCancelled),
				SubmittedDateTime:  to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, "2025-01-31T15:32:03.000Z"); return t }()),
				Submitter: &spark.Principal{
					Type: to.Ptr(spark.PrincipalTypeUser),
					ID:   to.Ptr("6f23a8a6-d954-4550-b91a-4df73ccd0311"),
				},
				TotalDuration: &spark.Duration{
					TimeUnit: to.Ptr(spark.TimeUnitSeconds),
					Value:    to.Ptr[float32](360),
				},
			}},
	}

	testsuite.serverFactory.LivySessionsServer.NewListLivySessionsPager = func(workspaceID string, options *spark.LivySessionsClientListLivySessionsOptions) (resp azfake.PagerResponder[spark.LivySessionsClientListLivySessionsResponse]) {
		testsuite.Require().Equal(exampleWorkspaceID, workspaceID)
		resp = azfake.PagerResponder[spark.LivySessionsClientListLivySessionsResponse]{}
		resp.AddPage(http.StatusOK, spark.LivySessionsClientListLivySessionsResponse{LivySessions: exampleRes}, nil)
		return
	}

	client := testsuite.clientFactory.NewLivySessionsClient()
	pager := client.NewListLivySessionsPager(exampleWorkspaceID, &spark.LivySessionsClientListLivySessionsOptions{ContinuationToken: nil})
	for pager.More() {
		nextResult, err := pager.NextPage(ctx)
		testsuite.Require().NoError(err, "Failed to advance page for example ")
		testsuite.Require().True(reflect.DeepEqual(exampleRes, nextResult.LivySessions))
		if err == nil {
			break
		}
	}
}
