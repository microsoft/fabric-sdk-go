// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See LICENSE in the project root for license information.
// Code generated by Microsoft (R) AutoRest Code Generator. DO NOT EDIT.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.
// SPDX-License-Identifier: MIT

package mlmodel

import (
	"context"
	"errors"
	"net/http"
	"net/url"
	"strings"

	"github.com/Azure/azure-sdk-for-go/sdk/azcore"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/policy"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/runtime"

	"github.com/microsoft/fabric-sdk-go/fabric/core"
	"github.com/microsoft/fabric-sdk-go/internal/iruntime"
	"github.com/microsoft/fabric-sdk-go/internal/pollers/locasync"
)

// ItemsClient contains the methods for the Items group.
// Don't use this type directly, use a constructor function instead.
type ItemsClient struct {
	internal *azcore.Client
	endpoint string
}

// BeginCreateMLModel - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// This API does not support create an machine learning model with definition.
// PERMISSIONS THE CALLER MUST HAVE CONTRIBUTOR OR HIGHER WORKSPACE ROLE.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS
// * To create a machine learning model the workspace must be on a supported Fabric capacity. For more information see: Microsoft
// Fabric license types
// [/fabric/enterprise/licenses#microsoft-fabric-license-types].
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - createMLModelRequest - Create item request payload.
//   - options - ItemsClientBeginCreateMLModelOptions contains the optional parameters for the ItemsClient.BeginCreateMLModel
//     method.
func (client *ItemsClient) BeginCreateMLModel(ctx context.Context, workspaceID string, createMLModelRequest CreateMLModelRequest, options *ItemsClientBeginCreateMLModelOptions) (*runtime.Poller[ItemsClientCreateMLModelResponse], error) {
	return client.beginCreateMLModel(ctx, workspaceID, createMLModelRequest, options)
}

// CreateMLModel - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// This API does not support create an machine learning model with definition.
// PERMISSIONS THE CALLER MUST HAVE CONTRIBUTOR OR HIGHER WORKSPACE ROLE.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS
// * To create a machine learning model the workspace must be on a supported Fabric capacity. For more information see: Microsoft
// Fabric license types
// [/fabric/enterprise/licenses#microsoft-fabric-license-types].
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *ItemsClient) createMLModel(ctx context.Context, workspaceID string, createMLModelRequest CreateMLModelRequest, options *ItemsClientBeginCreateMLModelOptions) (*http.Response, error) {
	var err error
	const operationName = "mlmodel.ItemsClient.BeginCreateMLModel"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.createMLModelCreateRequest(ctx, workspaceID, createMLModelRequest, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusCreated, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// createMLModelCreateRequest creates the CreateMLModel request.
func (client *ItemsClient) createMLModelCreateRequest(ctx context.Context, workspaceID string, createMLModelRequest CreateMLModelRequest, options *ItemsClientBeginCreateMLModelOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlModels"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, createMLModelRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// DeleteMLModel - PERMISSIONS The caller must have contributor or higher workspace role.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - mlModelID - The machine learning model ID.
//   - options - ItemsClientDeleteMLModelOptions contains the optional parameters for the ItemsClient.DeleteMLModel method.
func (client *ItemsClient) DeleteMLModel(ctx context.Context, workspaceID string, mlModelID string, options *ItemsClientDeleteMLModelOptions) (ItemsClientDeleteMLModelResponse, error) {
	var err error
	const operationName = "mlmodel.ItemsClient.DeleteMLModel"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.deleteMLModelCreateRequest(ctx, workspaceID, mlModelID, options)
	if err != nil {
		return ItemsClientDeleteMLModelResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return ItemsClientDeleteMLModelResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return ItemsClientDeleteMLModelResponse{}, err
	}
	return ItemsClientDeleteMLModelResponse{}, nil
}

// deleteMLModelCreateRequest creates the DeleteMLModel request.
func (client *ItemsClient) deleteMLModelCreateRequest(ctx context.Context, workspaceID string, mlModelID string, options *ItemsClientDeleteMLModelOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlModels/{mlModelId}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if mlModelID == "" {
		return nil, errors.New("parameter mlModelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{mlModelId}", url.PathEscape(mlModelID))
	req, err := runtime.NewRequest(ctx, http.MethodDelete, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// GetMLModel - PERMISSIONS The caller must have viewer or higher workspace role.
// REQUIRED DELEGATED SCOPES MLModel.Read.All or MLModel.ReadWrite.All or Item.Read.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - mlModelID - The machine learning model ID.
//   - options - ItemsClientGetMLModelOptions contains the optional parameters for the ItemsClient.GetMLModel method.
func (client *ItemsClient) GetMLModel(ctx context.Context, workspaceID string, mlModelID string, options *ItemsClientGetMLModelOptions) (ItemsClientGetMLModelResponse, error) {
	var err error
	const operationName = "mlmodel.ItemsClient.GetMLModel"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.getMLModelCreateRequest(ctx, workspaceID, mlModelID, options)
	if err != nil {
		return ItemsClientGetMLModelResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return ItemsClientGetMLModelResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return ItemsClientGetMLModelResponse{}, err
	}
	resp, err := client.getMLModelHandleResponse(httpResp)
	return resp, err
}

// getMLModelCreateRequest creates the GetMLModel request.
func (client *ItemsClient) getMLModelCreateRequest(ctx context.Context, workspaceID string, mlModelID string, options *ItemsClientGetMLModelOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlModels/{mlModelId}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if mlModelID == "" {
		return nil, errors.New("parameter mlModelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{mlModelId}", url.PathEscape(mlModelID))
	req, err := runtime.NewRequest(ctx, http.MethodGet, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// getMLModelHandleResponse handles the GetMLModel response.
func (client *ItemsClient) getMLModelHandleResponse(resp *http.Response) (ItemsClientGetMLModelResponse, error) {
	result := ItemsClientGetMLModelResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.MLModel); err != nil {
		return ItemsClientGetMLModelResponse{}, err
	}
	return result, nil
}

// NewListMLModelsPager - This API supports pagination [/rest/api/fabric/articles/pagination].
// PERMISSIONS The caller must have viewer or higher workspace role.
// REQUIRED DELEGATED SCOPES Workspace.Read.All or Workspace.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
// INTERFACE
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - options - ItemsClientListMLModelsOptions contains the optional parameters for the ItemsClient.NewListMLModelsPager method.
func (client *ItemsClient) NewListMLModelsPager(workspaceID string, options *ItemsClientListMLModelsOptions) *runtime.Pager[ItemsClientListMLModelsResponse] {
	return runtime.NewPager(runtime.PagingHandler[ItemsClientListMLModelsResponse]{
		More: func(page ItemsClientListMLModelsResponse) bool {
			return page.ContinuationURI != nil && len(*page.ContinuationURI) > 0
		},
		Fetcher: func(ctx context.Context, page *ItemsClientListMLModelsResponse) (ItemsClientListMLModelsResponse, error) {
			ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, "mlmodel.ItemsClient.NewListMLModelsPager")
			nextLink := ""
			if page != nil {
				nextLink = *page.ContinuationURI
			}
			resp, err := runtime.FetcherForNextLink(ctx, client.internal.Pipeline(), nextLink, func(ctx context.Context) (*policy.Request, error) {
				return client.listMLModelsCreateRequest(ctx, workspaceID, options)
			}, nil)
			if err != nil {
				return ItemsClientListMLModelsResponse{}, err
			}
			return client.listMLModelsHandleResponse(resp)
		},
		Tracer: client.internal.Tracer(),
	})
}

// listMLModelsCreateRequest creates the ListMLModels request.
func (client *ItemsClient) listMLModelsCreateRequest(ctx context.Context, workspaceID string, options *ItemsClientListMLModelsOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlModels"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	req, err := runtime.NewRequest(ctx, http.MethodGet, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	reqQP := req.Raw().URL.Query()
	if options != nil && options.ContinuationToken != nil {
		reqQP.Set("continuationToken", *options.ContinuationToken)
	}
	req.Raw().URL.RawQuery = reqQP.Encode()
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// listMLModelsHandleResponse handles the ListMLModels response.
func (client *ItemsClient) listMLModelsHandleResponse(resp *http.Response) (ItemsClientListMLModelsResponse, error) {
	result := ItemsClientListMLModelsResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.MLModels); err != nil {
		return ItemsClientListMLModelsResponse{}, err
	}
	return result, nil
}

// UpdateMLModel - PERMISSIONS The caller must have contributor or higher workspace role.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS
// * MLModel display name cannot be changed.
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - mlModelID - The machine learning model ID.
//   - updateMLModelRequest - Update machine learning model request payload.
//   - options - ItemsClientUpdateMLModelOptions contains the optional parameters for the ItemsClient.UpdateMLModel method.
func (client *ItemsClient) UpdateMLModel(ctx context.Context, workspaceID string, mlModelID string, updateMLModelRequest UpdateMLModelRequest, options *ItemsClientUpdateMLModelOptions) (ItemsClientUpdateMLModelResponse, error) {
	var err error
	const operationName = "mlmodel.ItemsClient.UpdateMLModel"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.updateMLModelCreateRequest(ctx, workspaceID, mlModelID, updateMLModelRequest, options)
	if err != nil {
		return ItemsClientUpdateMLModelResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return ItemsClientUpdateMLModelResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return ItemsClientUpdateMLModelResponse{}, err
	}
	resp, err := client.updateMLModelHandleResponse(httpResp)
	return resp, err
}

// updateMLModelCreateRequest creates the UpdateMLModel request.
func (client *ItemsClient) updateMLModelCreateRequest(ctx context.Context, workspaceID string, mlModelID string, updateMLModelRequest UpdateMLModelRequest, options *ItemsClientUpdateMLModelOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlModels/{mlModelId}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if mlModelID == "" {
		return nil, errors.New("parameter mlModelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{mlModelId}", url.PathEscape(mlModelID))
	req, err := runtime.NewRequest(ctx, http.MethodPatch, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, updateMLModelRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// updateMLModelHandleResponse handles the UpdateMLModel response.
func (client *ItemsClient) updateMLModelHandleResponse(resp *http.Response) (ItemsClientUpdateMLModelResponse, error) {
	result := ItemsClientUpdateMLModelResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.MLModel); err != nil {
		return ItemsClientUpdateMLModelResponse{}, err
	}
	return result, nil
}

// Custom code starts below

// CreateMLModel - returns ItemsClientCreateMLModelResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// This API does not support create an machine learning model with definition.
//
// PERMISSIONS THE CALLER MUST HAVE CONTRIBUTOR OR HIGHER WORKSPACE ROLE.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
//
// LIMITATIONS
//
//   - To create a machine learning model the workspace must be on a supported Fabric capacity. For more information see: Microsoft Fabric license types
//     [/fabric/enterprise/licenses#microsoft-fabric-license-types].
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
//
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - createMLModelRequest - Create item request payload.
//   - options - ItemsClientBeginCreateMLModelOptions contains the optional parameters for the ItemsClient.BeginCreateMLModel method.
func (client *ItemsClient) CreateMLModel(ctx context.Context, workspaceID string, createMLModelRequest CreateMLModelRequest, options *ItemsClientBeginCreateMLModelOptions) (ItemsClientCreateMLModelResponse, error) {
	return iruntime.NewLRO(client.BeginCreateMLModel(ctx, workspaceID, createMLModelRequest, options)).Sync(ctx)
}

// beginCreateMLModel creates the createMLModel request.
func (client *ItemsClient) beginCreateMLModel(ctx context.Context, workspaceID string, createMLModelRequest CreateMLModelRequest, options *ItemsClientBeginCreateMLModelOptions) (*runtime.Poller[ItemsClientCreateMLModelResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.createMLModel(ctx, workspaceID, createMLModelRequest, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[ItemsClientCreateMLModelResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[ItemsClientCreateMLModelResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[ItemsClientCreateMLModelResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[ItemsClientCreateMLModelResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// ListMLModels - returns array of MLModel from all pages.
// This API supports pagination [/rest/api/fabric/articles/pagination].
//
// PERMISSIONS The caller must have viewer or higher workspace role.
//
// # REQUIRED DELEGATED SCOPES Workspace.Read.All or Workspace.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] | No | | Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | No |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - options - ItemsClientListMLModelsOptions contains the optional parameters for the ItemsClient.NewListMLModelsPager method.
func (client *ItemsClient) ListMLModels(ctx context.Context, workspaceID string, options *ItemsClientListMLModelsOptions) ([]MLModel, error) {
	pager := client.NewListMLModelsPager(workspaceID, options)
	mapper := func(resp ItemsClientListMLModelsResponse) []MLModel {
		return resp.Value
	}
	list, err := iruntime.NewPageIterator(ctx, pager, mapper).Get()
	if err != nil {
		return nil, err
	}
	return list, nil
}
