// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See LICENSE in the project root for license information.
// Code generated by Microsoft (R) AutoRest Code Generator. DO NOT EDIT.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.
// SPDX-License-Identifier: MIT

package mlmodel

import (
	"context"
	"errors"
	"net/http"
	"net/url"
	"strings"

	"github.com/Azure/azure-sdk-for-go/sdk/azcore"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/policy"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/runtime"

	"github.com/microsoft/fabric-sdk-go/fabric/core"
	"github.com/microsoft/fabric-sdk-go/internal/iruntime"
	"github.com/microsoft/fabric-sdk-go/internal/pollers/locasync"
)

// EndpointClient contains the methods for the Endpoint group.
// Don't use this type directly, use a constructor function instead.
type EndpointClient struct {
	internal *azcore.Client
	endpoint string
}

// BeginActivateMLModelEndpointVersion - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - options - EndpointClientBeginActivateMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.BeginActivateMLModelEndpointVersion
//     method.
func (client *EndpointClient) BeginActivateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginActivateMLModelEndpointVersionOptions) (*runtime.Poller[EndpointClientActivateMLModelEndpointVersionResponse], error) {
	return client.beginActivateMLModelEndpointVersion(ctx, workspaceID, modelID, name, options)
}

// ActivateMLModelEndpointVersion - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *EndpointClient) activateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginActivateMLModelEndpointVersionOptions) (*http.Response, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.BeginActivateMLModelEndpointVersion"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.activateMLModelEndpointVersionCreateRequest(ctx, workspaceID, modelID, name, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// activateMLModelEndpointVersionCreateRequest creates the ActivateMLModelEndpointVersion request.
func (client *EndpointClient) activateMLModelEndpointVersionCreateRequest(ctx context.Context, workspaceID string, modelID string, name string, _ *EndpointClientBeginActivateMLModelEndpointVersionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint/versions/{name}/activate"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	if name == "" {
		return nil, errors.New("parameter name cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{name}", url.PathEscape(name))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// BeginDeactivateAllMLModelEndpointVersions - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - options - EndpointClientBeginDeactivateAllMLModelEndpointVersionsOptions contains the optional parameters for the EndpointClient.BeginDeactivateAllMLModelEndpointVersions
//     method.
func (client *EndpointClient) BeginDeactivateAllMLModelEndpointVersions(ctx context.Context, workspaceID string, modelID string, options *EndpointClientBeginDeactivateAllMLModelEndpointVersionsOptions) (*runtime.Poller[EndpointClientDeactivateAllMLModelEndpointVersionsResponse], error) {
	return client.beginDeactivateAllMLModelEndpointVersions(ctx, workspaceID, modelID, options)
}

// DeactivateAllMLModelEndpointVersions - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *EndpointClient) deactivateAllMLModelEndpointVersions(ctx context.Context, workspaceID string, modelID string, options *EndpointClientBeginDeactivateAllMLModelEndpointVersionsOptions) (*http.Response, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.BeginDeactivateAllMLModelEndpointVersions"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.deactivateAllMLModelEndpointVersionsCreateRequest(ctx, workspaceID, modelID, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// deactivateAllMLModelEndpointVersionsCreateRequest creates the DeactivateAllMLModelEndpointVersions request.
func (client *EndpointClient) deactivateAllMLModelEndpointVersionsCreateRequest(ctx context.Context, workspaceID string, modelID string, _ *EndpointClientBeginDeactivateAllMLModelEndpointVersionsOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint/versions/deactivateAll"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// BeginDeactivateMLModelEndpointVersion - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - options - EndpointClientBeginDeactivateMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.BeginDeactivateMLModelEndpointVersion
//     method.
func (client *EndpointClient) BeginDeactivateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginDeactivateMLModelEndpointVersionOptions) (*runtime.Poller[EndpointClientDeactivateMLModelEndpointVersionResponse], error) {
	return client.beginDeactivateMLModelEndpointVersion(ctx, workspaceID, modelID, name, options)
}

// DeactivateMLModelEndpointVersion - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *EndpointClient) deactivateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginDeactivateMLModelEndpointVersionOptions) (*http.Response, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.BeginDeactivateMLModelEndpointVersion"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.deactivateMLModelEndpointVersionCreateRequest(ctx, workspaceID, modelID, name, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// deactivateMLModelEndpointVersionCreateRequest creates the DeactivateMLModelEndpointVersion request.
func (client *EndpointClient) deactivateMLModelEndpointVersionCreateRequest(ctx context.Context, workspaceID string, modelID string, name string, _ *EndpointClientBeginDeactivateMLModelEndpointVersionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint/versions/{name}/deactivate"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	if name == "" {
		return nil, errors.New("parameter name cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{name}", url.PathEscape(name))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// GetMLModelEndpoint - PERMISSIONS The caller must have read permission on the MLModel.
// REQUIRED DELEGATED SCOPES MLModel.Read.All or MLModel.ReadWrite.All or Item.Read.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - options - EndpointClientGetMLModelEndpointOptions contains the optional parameters for the EndpointClient.GetMLModelEndpoint
//     method.
func (client *EndpointClient) GetMLModelEndpoint(ctx context.Context, workspaceID string, modelID string, options *EndpointClientGetMLModelEndpointOptions) (EndpointClientGetMLModelEndpointResponse, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.GetMLModelEndpoint"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.getMLModelEndpointCreateRequest(ctx, workspaceID, modelID, options)
	if err != nil {
		return EndpointClientGetMLModelEndpointResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return EndpointClientGetMLModelEndpointResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return EndpointClientGetMLModelEndpointResponse{}, err
	}
	resp, err := client.getMLModelEndpointHandleResponse(httpResp)
	return resp, err
}

// getMLModelEndpointCreateRequest creates the GetMLModelEndpoint request.
func (client *EndpointClient) getMLModelEndpointCreateRequest(ctx context.Context, workspaceID string, modelID string, _ *EndpointClientGetMLModelEndpointOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	req, err := runtime.NewRequest(ctx, http.MethodGet, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// getMLModelEndpointHandleResponse handles the GetMLModelEndpoint response.
func (client *EndpointClient) getMLModelEndpointHandleResponse(resp *http.Response) (EndpointClientGetMLModelEndpointResponse, error) {
	result := EndpointClientGetMLModelEndpointResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.Endpoint); err != nil {
		return EndpointClientGetMLModelEndpointResponse{}, err
	}
	return result, nil
}

// GetMLModelEndpointVersion - For machine learning model versions where endpoints have never been enabled, or have been disabled
// after enabling, the status would be 'deactivated'.
// PERMISSIONS The caller must have read permission on the MLModel.
// REQUIRED DELEGATED SCOPES MLModel.Read.All or MLModel.ReadWrite.All or Item.Read.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - options - EndpointClientGetMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.GetMLModelEndpointVersion
//     method.
func (client *EndpointClient) GetMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientGetMLModelEndpointVersionOptions) (EndpointClientGetMLModelEndpointVersionResponse, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.GetMLModelEndpointVersion"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.getMLModelEndpointVersionCreateRequest(ctx, workspaceID, modelID, name, options)
	if err != nil {
		return EndpointClientGetMLModelEndpointVersionResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return EndpointClientGetMLModelEndpointVersionResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return EndpointClientGetMLModelEndpointVersionResponse{}, err
	}
	resp, err := client.getMLModelEndpointVersionHandleResponse(httpResp)
	return resp, err
}

// getMLModelEndpointVersionCreateRequest creates the GetMLModelEndpointVersion request.
func (client *EndpointClient) getMLModelEndpointVersionCreateRequest(ctx context.Context, workspaceID string, modelID string, name string, _ *EndpointClientGetMLModelEndpointVersionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint/versions/{name}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	if name == "" {
		return nil, errors.New("parameter name cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{name}", url.PathEscape(name))
	req, err := runtime.NewRequest(ctx, http.MethodGet, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// getMLModelEndpointVersionHandleResponse handles the GetMLModelEndpointVersion response.
func (client *EndpointClient) getMLModelEndpointVersionHandleResponse(resp *http.Response) (EndpointClientGetMLModelEndpointVersionResponse, error) {
	result := EndpointClientGetMLModelEndpointVersionResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.EndpointVersionInfo); err != nil {
		return EndpointClientGetMLModelEndpointVersionResponse{}, err
	}
	return result, nil
}

// NewListMLModelEndpointVersionsPager - This API supports pagination [/rest/api/fabric/articles/pagination].
// PERMISSIONS The caller must have read permission on the MLModel.
// REQUIRED DELEGATED SCOPES MLModel.Read.All or MLModel.ReadWrite.All or Item.Read.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - options - EndpointClientListMLModelEndpointVersionsOptions contains the optional parameters for the EndpointClient.NewListMLModelEndpointVersionsPager
//     method.
func (client *EndpointClient) NewListMLModelEndpointVersionsPager(workspaceID string, modelID string, options *EndpointClientListMLModelEndpointVersionsOptions) *runtime.Pager[EndpointClientListMLModelEndpointVersionsResponse] {
	return runtime.NewPager(runtime.PagingHandler[EndpointClientListMLModelEndpointVersionsResponse]{
		More: func(page EndpointClientListMLModelEndpointVersionsResponse) bool {
			return page.ContinuationURI != nil && len(*page.ContinuationURI) > 0
		},
		Fetcher: func(ctx context.Context, page *EndpointClientListMLModelEndpointVersionsResponse) (EndpointClientListMLModelEndpointVersionsResponse, error) {
			ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, "mlmodel.EndpointClient.NewListMLModelEndpointVersionsPager")
			nextLink := ""
			if page != nil {
				nextLink = *page.ContinuationURI
			}
			resp, err := runtime.FetcherForNextLink(ctx, client.internal.Pipeline(), nextLink, func(ctx context.Context) (*policy.Request, error) {
				return client.listMLModelEndpointVersionsCreateRequest(ctx, workspaceID, modelID, options)
			}, nil)
			if err != nil {
				return EndpointClientListMLModelEndpointVersionsResponse{}, err
			}
			return client.listMLModelEndpointVersionsHandleResponse(resp)
		},
		Tracer: client.internal.Tracer(),
	})
}

// listMLModelEndpointVersionsCreateRequest creates the ListMLModelEndpointVersions request.
func (client *EndpointClient) listMLModelEndpointVersionsCreateRequest(ctx context.Context, workspaceID string, modelID string, options *EndpointClientListMLModelEndpointVersionsOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint/versions"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	req, err := runtime.NewRequest(ctx, http.MethodGet, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	reqQP := req.Raw().URL.Query()
	if options != nil && options.ContinuationToken != nil {
		reqQP.Set("continuationToken", *options.ContinuationToken)
	}
	req.Raw().URL.RawQuery = reqQP.Encode()
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// listMLModelEndpointVersionsHandleResponse handles the ListMLModelEndpointVersions response.
func (client *EndpointClient) listMLModelEndpointVersionsHandleResponse(resp *http.Response) (EndpointClientListMLModelEndpointVersionsResponse, error) {
	result := EndpointClientListMLModelEndpointVersionsResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.EndpointVersions); err != nil {
		return EndpointClientListMLModelEndpointVersionsResponse{}, err
	}
	return result, nil
}

// BeginScoreMLModelEndpoint - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - scoreDataRequest - The data to score with the model.
//   - options - EndpointClientBeginScoreMLModelEndpointOptions contains the optional parameters for the EndpointClient.BeginScoreMLModelEndpoint
//     method.
func (client *EndpointClient) BeginScoreMLModelEndpoint(ctx context.Context, workspaceID string, modelID string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointOptions) (*runtime.Poller[EndpointClientScoreMLModelEndpointResponse], error) {
	return client.beginScoreMLModelEndpoint(ctx, workspaceID, modelID, scoreDataRequest, options)
}

// ScoreMLModelEndpoint - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *EndpointClient) scoreMLModelEndpoint(ctx context.Context, workspaceID string, modelID string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointOptions) (*http.Response, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.BeginScoreMLModelEndpoint"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.scoreMLModelEndpointCreateRequest(ctx, workspaceID, modelID, scoreDataRequest, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// scoreMLModelEndpointCreateRequest creates the ScoreMLModelEndpoint request.
func (client *EndpointClient) scoreMLModelEndpointCreateRequest(ctx context.Context, workspaceID string, modelID string, scoreDataRequest ScoreDataRequest, _ *EndpointClientBeginScoreMLModelEndpointOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlModels/{modelId}/endpoint/score"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, scoreDataRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// BeginScoreMLModelEndpointVersion - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - scoreDataRequest - The data to score with the model.
//   - options - EndpointClientBeginScoreMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.BeginScoreMLModelEndpointVersion
//     method.
func (client *EndpointClient) BeginScoreMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointVersionOptions) (*runtime.Poller[EndpointClientScoreMLModelEndpointVersionResponse], error) {
	return client.beginScoreMLModelEndpointVersion(ctx, workspaceID, modelID, name, scoreDataRequest, options)
}

// ScoreMLModelEndpointVersion - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *EndpointClient) scoreMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointVersionOptions) (*http.Response, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.BeginScoreMLModelEndpointVersion"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.scoreMLModelEndpointVersionCreateRequest(ctx, workspaceID, modelID, name, scoreDataRequest, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// scoreMLModelEndpointVersionCreateRequest creates the ScoreMLModelEndpointVersion request.
func (client *EndpointClient) scoreMLModelEndpointVersionCreateRequest(ctx context.Context, workspaceID string, modelID string, name string, scoreDataRequest ScoreDataRequest, _ *EndpointClientBeginScoreMLModelEndpointVersionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint/versions/{name}/score"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	if name == "" {
		return nil, errors.New("parameter name cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{name}", url.PathEscape(name))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, scoreDataRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// UpdateMLModelEndpoint - PERMISSIONS The caller must have write permission on the MLModel.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - updateMLModelEndpointRequest - Update MLModel endpoint request payload.
//   - options - EndpointClientUpdateMLModelEndpointOptions contains the optional parameters for the EndpointClient.UpdateMLModelEndpoint
//     method.
func (client *EndpointClient) UpdateMLModelEndpoint(ctx context.Context, workspaceID string, modelID string, updateMLModelEndpointRequest UpdateMLModelEndpointRequest, options *EndpointClientUpdateMLModelEndpointOptions) (EndpointClientUpdateMLModelEndpointResponse, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.UpdateMLModelEndpoint"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.updateMLModelEndpointCreateRequest(ctx, workspaceID, modelID, updateMLModelEndpointRequest, options)
	if err != nil {
		return EndpointClientUpdateMLModelEndpointResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return EndpointClientUpdateMLModelEndpointResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return EndpointClientUpdateMLModelEndpointResponse{}, err
	}
	resp, err := client.updateMLModelEndpointHandleResponse(httpResp)
	return resp, err
}

// updateMLModelEndpointCreateRequest creates the UpdateMLModelEndpoint request.
func (client *EndpointClient) updateMLModelEndpointCreateRequest(ctx context.Context, workspaceID string, modelID string, updateMLModelEndpointRequest UpdateMLModelEndpointRequest, _ *EndpointClientUpdateMLModelEndpointOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	req, err := runtime.NewRequest(ctx, http.MethodPatch, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, updateMLModelEndpointRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// updateMLModelEndpointHandleResponse handles the UpdateMLModelEndpoint response.
func (client *EndpointClient) updateMLModelEndpointHandleResponse(resp *http.Response) (EndpointClientUpdateMLModelEndpointResponse, error) {
	result := EndpointClientUpdateMLModelEndpointResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.Endpoint); err != nil {
		return EndpointClientUpdateMLModelEndpointResponse{}, err
	}
	return result, nil
}

// UpdateMLModelEndpointVersion - PERMISSIONS The caller must have write permission on the MLModel.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS
// * Only machine learning model endpoint versions which are enabled can be updated.
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - updateMLModelEndpointVersionRequest - MLModel endpoint configuration update request payload.
//   - options - EndpointClientUpdateMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.UpdateMLModelEndpointVersion
//     method.
func (client *EndpointClient) UpdateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, updateMLModelEndpointVersionRequest UpdateMLModelEndpointVersionRequest, options *EndpointClientUpdateMLModelEndpointVersionOptions) (EndpointClientUpdateMLModelEndpointVersionResponse, error) {
	var err error
	const operationName = "mlmodel.EndpointClient.UpdateMLModelEndpointVersion"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.updateMLModelEndpointVersionCreateRequest(ctx, workspaceID, modelID, name, updateMLModelEndpointVersionRequest, options)
	if err != nil {
		return EndpointClientUpdateMLModelEndpointVersionResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return EndpointClientUpdateMLModelEndpointVersionResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return EndpointClientUpdateMLModelEndpointVersionResponse{}, err
	}
	resp, err := client.updateMLModelEndpointVersionHandleResponse(httpResp)
	return resp, err
}

// updateMLModelEndpointVersionCreateRequest creates the UpdateMLModelEndpointVersion request.
func (client *EndpointClient) updateMLModelEndpointVersionCreateRequest(ctx context.Context, workspaceID string, modelID string, name string, updateMLModelEndpointVersionRequest UpdateMLModelEndpointVersionRequest, _ *EndpointClientUpdateMLModelEndpointVersionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/mlmodels/{modelId}/endpoint/versions/{name}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if modelID == "" {
		return nil, errors.New("parameter modelID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{modelId}", url.PathEscape(modelID))
	if name == "" {
		return nil, errors.New("parameter name cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{name}", url.PathEscape(name))
	req, err := runtime.NewRequest(ctx, http.MethodPatch, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, updateMLModelEndpointVersionRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// updateMLModelEndpointVersionHandleResponse handles the UpdateMLModelEndpointVersion response.
func (client *EndpointClient) updateMLModelEndpointVersionHandleResponse(resp *http.Response) (EndpointClientUpdateMLModelEndpointVersionResponse, error) {
	result := EndpointClientUpdateMLModelEndpointVersionResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.EndpointVersionInfo); err != nil {
		return EndpointClientUpdateMLModelEndpointVersionResponse{}, err
	}
	return result, nil
}

// Custom code starts below

// ActivateMLModelEndpointVersion - returns EndpointClientActivateMLModelEndpointVersionResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - options - EndpointClientBeginActivateMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.BeginActivateMLModelEndpointVersion method.
func (client *EndpointClient) ActivateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginActivateMLModelEndpointVersionOptions) (EndpointClientActivateMLModelEndpointVersionResponse, error) {
	result, err := iruntime.NewLRO(client.BeginActivateMLModelEndpointVersion(ctx, workspaceID, modelID, name, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return EndpointClientActivateMLModelEndpointVersionResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return EndpointClientActivateMLModelEndpointVersionResponse{}, err
	}
	return result, err
}

// beginActivateMLModelEndpointVersion creates the activateMLModelEndpointVersion request.
func (client *EndpointClient) beginActivateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginActivateMLModelEndpointVersionOptions) (*runtime.Poller[EndpointClientActivateMLModelEndpointVersionResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.activateMLModelEndpointVersion(ctx, workspaceID, modelID, name, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[EndpointClientActivateMLModelEndpointVersionResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[EndpointClientActivateMLModelEndpointVersionResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[EndpointClientActivateMLModelEndpointVersionResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[EndpointClientActivateMLModelEndpointVersionResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// DeactivateAllMLModelEndpointVersions - returns EndpointClientDeactivateAllMLModelEndpointVersionsResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - options - EndpointClientBeginDeactivateAllMLModelEndpointVersionsOptions contains the optional parameters for the EndpointClient.BeginDeactivateAllMLModelEndpointVersions method.
func (client *EndpointClient) DeactivateAllMLModelEndpointVersions(ctx context.Context, workspaceID string, modelID string, options *EndpointClientBeginDeactivateAllMLModelEndpointVersionsOptions) (EndpointClientDeactivateAllMLModelEndpointVersionsResponse, error) {
	result, err := iruntime.NewLRO(client.BeginDeactivateAllMLModelEndpointVersions(ctx, workspaceID, modelID, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return EndpointClientDeactivateAllMLModelEndpointVersionsResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return EndpointClientDeactivateAllMLModelEndpointVersionsResponse{}, err
	}
	return result, err
}

// beginDeactivateAllMLModelEndpointVersions creates the deactivateAllMLModelEndpointVersions request.
func (client *EndpointClient) beginDeactivateAllMLModelEndpointVersions(ctx context.Context, workspaceID string, modelID string, options *EndpointClientBeginDeactivateAllMLModelEndpointVersionsOptions) (*runtime.Poller[EndpointClientDeactivateAllMLModelEndpointVersionsResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.deactivateAllMLModelEndpointVersions(ctx, workspaceID, modelID, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[EndpointClientDeactivateAllMLModelEndpointVersionsResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[EndpointClientDeactivateAllMLModelEndpointVersionsResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[EndpointClientDeactivateAllMLModelEndpointVersionsResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[EndpointClientDeactivateAllMLModelEndpointVersionsResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// DeactivateMLModelEndpointVersion - returns EndpointClientDeactivateMLModelEndpointVersionResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - options - EndpointClientBeginDeactivateMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.BeginDeactivateMLModelEndpointVersion method.
func (client *EndpointClient) DeactivateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginDeactivateMLModelEndpointVersionOptions) (EndpointClientDeactivateMLModelEndpointVersionResponse, error) {
	result, err := iruntime.NewLRO(client.BeginDeactivateMLModelEndpointVersion(ctx, workspaceID, modelID, name, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return EndpointClientDeactivateMLModelEndpointVersionResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return EndpointClientDeactivateMLModelEndpointVersionResponse{}, err
	}
	return result, err
}

// beginDeactivateMLModelEndpointVersion creates the deactivateMLModelEndpointVersion request.
func (client *EndpointClient) beginDeactivateMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, options *EndpointClientBeginDeactivateMLModelEndpointVersionOptions) (*runtime.Poller[EndpointClientDeactivateMLModelEndpointVersionResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.deactivateMLModelEndpointVersion(ctx, workspaceID, modelID, name, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[EndpointClientDeactivateMLModelEndpointVersionResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[EndpointClientDeactivateMLModelEndpointVersionResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[EndpointClientDeactivateMLModelEndpointVersionResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[EndpointClientDeactivateMLModelEndpointVersionResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// ScoreMLModelEndpoint - returns EndpointClientScoreMLModelEndpointResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - scoreDataRequest - The data to score with the model.
//   - options - EndpointClientBeginScoreMLModelEndpointOptions contains the optional parameters for the EndpointClient.BeginScoreMLModelEndpoint method.
func (client *EndpointClient) ScoreMLModelEndpoint(ctx context.Context, workspaceID string, modelID string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointOptions) (EndpointClientScoreMLModelEndpointResponse, error) {
	result, err := iruntime.NewLRO(client.BeginScoreMLModelEndpoint(ctx, workspaceID, modelID, scoreDataRequest, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return EndpointClientScoreMLModelEndpointResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return EndpointClientScoreMLModelEndpointResponse{}, err
	}
	return result, err
}

// beginScoreMLModelEndpoint creates the scoreMLModelEndpoint request.
func (client *EndpointClient) beginScoreMLModelEndpoint(ctx context.Context, workspaceID string, modelID string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointOptions) (*runtime.Poller[EndpointClientScoreMLModelEndpointResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.scoreMLModelEndpoint(ctx, workspaceID, modelID, scoreDataRequest, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[EndpointClientScoreMLModelEndpointResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[EndpointClientScoreMLModelEndpointResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[EndpointClientScoreMLModelEndpointResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[EndpointClientScoreMLModelEndpointResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// ScoreMLModelEndpointVersion - returns EndpointClientScoreMLModelEndpointVersionResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// PERMISSIONS THE CALLER MUST HAVE WRITE PERMISSION ON THE MLMODEL.
// REQUIRED DELEGATED SCOPES MLModel.ReadWrite.All or Item.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - name - The MLModel version name.
//   - scoreDataRequest - The data to score with the model.
//   - options - EndpointClientBeginScoreMLModelEndpointVersionOptions contains the optional parameters for the EndpointClient.BeginScoreMLModelEndpointVersion method.
func (client *EndpointClient) ScoreMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointVersionOptions) (EndpointClientScoreMLModelEndpointVersionResponse, error) {
	result, err := iruntime.NewLRO(client.BeginScoreMLModelEndpointVersion(ctx, workspaceID, modelID, name, scoreDataRequest, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return EndpointClientScoreMLModelEndpointVersionResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return EndpointClientScoreMLModelEndpointVersionResponse{}, err
	}
	return result, err
}

// beginScoreMLModelEndpointVersion creates the scoreMLModelEndpointVersion request.
func (client *EndpointClient) beginScoreMLModelEndpointVersion(ctx context.Context, workspaceID string, modelID string, name string, scoreDataRequest ScoreDataRequest, options *EndpointClientBeginScoreMLModelEndpointVersionOptions) (*runtime.Poller[EndpointClientScoreMLModelEndpointVersionResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.scoreMLModelEndpointVersion(ctx, workspaceID, modelID, name, scoreDataRequest, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[EndpointClientScoreMLModelEndpointVersionResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[EndpointClientScoreMLModelEndpointVersionResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[EndpointClientScoreMLModelEndpointVersionResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[EndpointClientScoreMLModelEndpointVersionResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// ListMLModelEndpointVersions - returns array of EndpointVersionInfo from all pages.
// This API supports pagination [/rest/api/fabric/articles/pagination].
//
// PERMISSIONS The caller must have read permission on the MLModel.
//
// # REQUIRED DELEGATED SCOPES MLModel.Read.All or MLModel.ReadWrite.All or Item.Read.All or Item.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - modelID - The machine learning model ID.
//   - options - EndpointClientListMLModelEndpointVersionsOptions contains the optional parameters for the EndpointClient.NewListMLModelEndpointVersionsPager method.
func (client *EndpointClient) ListMLModelEndpointVersions(ctx context.Context, workspaceID string, modelID string, options *EndpointClientListMLModelEndpointVersionsOptions) ([]EndpointVersionInfo, error) {
	pager := client.NewListMLModelEndpointVersionsPager(workspaceID, modelID, options)
	mapper := func(resp EndpointClientListMLModelEndpointVersionsResponse) []EndpointVersionInfo {
		return resp.Value
	}
	list, err := iruntime.NewPageIterator(ctx, pager, mapper).Get()
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return []EndpointVersionInfo{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return []EndpointVersionInfo{}, err
	}
	return list, nil
}
