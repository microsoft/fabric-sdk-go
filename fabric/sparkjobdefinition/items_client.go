// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See LICENSE in the project root for license information.
// Code generated by Microsoft (R) AutoRest Code Generator. DO NOT EDIT.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.
// SPDX-License-Identifier: MIT

package sparkjobdefinition

import (
	"context"
	"errors"
	"net/http"
	"net/url"
	"strconv"
	"strings"

	"github.com/Azure/azure-sdk-for-go/sdk/azcore"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/policy"
	"github.com/Azure/azure-sdk-for-go/sdk/azcore/runtime"

	"github.com/microsoft/fabric-sdk-go/fabric/core"
	"github.com/microsoft/fabric-sdk-go/internal/iruntime"
	"github.com/microsoft/fabric-sdk-go/internal/pollers/locasync"
)

// ItemsClient contains the methods for the Items group.
// Don't use this type directly, use a constructor function instead.
type ItemsClient struct {
	internal *azcore.Client
	endpoint string
}

// BeginCreateSparkJobDefinition - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// To create spark job definition with a public definition, refer to SparkJobDefinitionV1 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v1]
// article for SparkJobDefinitionV1
// format and SparkJobDefinitionV2 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v2] article
// for SparkJobDefinitionV2 format.
// PERMISSIONS The caller must have a contributor workspace role.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS
// * To create a spark job definition the workspace must be on a supported Fabric capacity. For more information see: Microsoft
// Fabric license types
// [/fabric/enterprise/licenses#microsoft-fabric-license-types].
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - createSparkJobDefinitionRequest - Create item request payload.
//   - options - ItemsClientBeginCreateSparkJobDefinitionOptions contains the optional parameters for the ItemsClient.BeginCreateSparkJobDefinition
//     method.
func (client *ItemsClient) BeginCreateSparkJobDefinition(ctx context.Context, workspaceID string, createSparkJobDefinitionRequest CreateSparkJobDefinitionRequest, options *ItemsClientBeginCreateSparkJobDefinitionOptions) (*runtime.Poller[ItemsClientCreateSparkJobDefinitionResponse], error) {
	return client.beginCreateSparkJobDefinition(ctx, workspaceID, createSparkJobDefinitionRequest, options)
}

// CreateSparkJobDefinition - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// To create spark job definition with a public definition, refer to SparkJobDefinitionV1 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v1]
// article for SparkJobDefinitionV1
// format and SparkJobDefinitionV2 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v2] article
// for SparkJobDefinitionV2 format.
// PERMISSIONS The caller must have a contributor workspace role.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS
// * To create a spark job definition the workspace must be on a supported Fabric capacity. For more information see: Microsoft
// Fabric license types
// [/fabric/enterprise/licenses#microsoft-fabric-license-types].
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *ItemsClient) createSparkJobDefinition(ctx context.Context, workspaceID string, createSparkJobDefinitionRequest CreateSparkJobDefinitionRequest, options *ItemsClientBeginCreateSparkJobDefinitionOptions) (*http.Response, error) {
	var err error
	const operationName = "sparkjobdefinition.ItemsClient.BeginCreateSparkJobDefinition"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.createSparkJobDefinitionCreateRequest(ctx, workspaceID, createSparkJobDefinitionRequest, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusCreated, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// createSparkJobDefinitionCreateRequest creates the CreateSparkJobDefinition request.
func (client *ItemsClient) createSparkJobDefinitionCreateRequest(ctx context.Context, workspaceID string, createSparkJobDefinitionRequest CreateSparkJobDefinitionRequest, _ *ItemsClientBeginCreateSparkJobDefinitionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/sparkJobDefinitions"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, createSparkJobDefinitionRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// DeleteSparkJobDefinition - PERMISSIONS The caller must have write permissions for the spark job definition.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - sparkJobDefinitionID - The spark job definition ID.
//   - options - ItemsClientDeleteSparkJobDefinitionOptions contains the optional parameters for the ItemsClient.DeleteSparkJobDefinition
//     method.
func (client *ItemsClient) DeleteSparkJobDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, options *ItemsClientDeleteSparkJobDefinitionOptions) (ItemsClientDeleteSparkJobDefinitionResponse, error) {
	var err error
	const operationName = "sparkjobdefinition.ItemsClient.DeleteSparkJobDefinition"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.deleteSparkJobDefinitionCreateRequest(ctx, workspaceID, sparkJobDefinitionID, options)
	if err != nil {
		return ItemsClientDeleteSparkJobDefinitionResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return ItemsClientDeleteSparkJobDefinitionResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return ItemsClientDeleteSparkJobDefinitionResponse{}, err
	}
	return ItemsClientDeleteSparkJobDefinitionResponse{}, nil
}

// deleteSparkJobDefinitionCreateRequest creates the DeleteSparkJobDefinition request.
func (client *ItemsClient) deleteSparkJobDefinitionCreateRequest(ctx context.Context, workspaceID string, sparkJobDefinitionID string, _ *ItemsClientDeleteSparkJobDefinitionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if sparkJobDefinitionID == "" {
		return nil, errors.New("parameter sparkJobDefinitionID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{sparkJobDefinitionId}", url.PathEscape(sparkJobDefinitionID))
	req, err := runtime.NewRequest(ctx, http.MethodDelete, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// GetSparkJobDefinition - PERMISSIONS The caller must have read permissions for the spark job definition.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.Read.All or SparkJobDefinition.ReadWrite.All or Item.Read.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - sparkJobDefinitionID - The spark job definition ID.
//   - options - ItemsClientGetSparkJobDefinitionOptions contains the optional parameters for the ItemsClient.GetSparkJobDefinition
//     method.
func (client *ItemsClient) GetSparkJobDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, options *ItemsClientGetSparkJobDefinitionOptions) (ItemsClientGetSparkJobDefinitionResponse, error) {
	var err error
	const operationName = "sparkjobdefinition.ItemsClient.GetSparkJobDefinition"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.getSparkJobDefinitionCreateRequest(ctx, workspaceID, sparkJobDefinitionID, options)
	if err != nil {
		return ItemsClientGetSparkJobDefinitionResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return ItemsClientGetSparkJobDefinitionResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return ItemsClientGetSparkJobDefinitionResponse{}, err
	}
	resp, err := client.getSparkJobDefinitionHandleResponse(httpResp)
	return resp, err
}

// getSparkJobDefinitionCreateRequest creates the GetSparkJobDefinition request.
func (client *ItemsClient) getSparkJobDefinitionCreateRequest(ctx context.Context, workspaceID string, sparkJobDefinitionID string, _ *ItemsClientGetSparkJobDefinitionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if sparkJobDefinitionID == "" {
		return nil, errors.New("parameter sparkJobDefinitionID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{sparkJobDefinitionId}", url.PathEscape(sparkJobDefinitionID))
	req, err := runtime.NewRequest(ctx, http.MethodGet, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// getSparkJobDefinitionHandleResponse handles the GetSparkJobDefinition response.
func (client *ItemsClient) getSparkJobDefinitionHandleResponse(resp *http.Response) (ItemsClientGetSparkJobDefinitionResponse, error) {
	result := ItemsClientGetSparkJobDefinitionResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.SparkJobDefinition); err != nil {
		return ItemsClientGetSparkJobDefinitionResponse{}, err
	}
	return result, nil
}

// BeginGetSparkJobDefinitionDefinition - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// When you get a spark job definition's public definition, the sensitivity label is not a part of the definition.
// PERMISSIONS The caller must have read and write permissions for the spark job definition.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS This API is blocked for a spark job definition with an encrypted sensitivity label.
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - sparkJobDefinitionID - The spark job definition ID.
//   - options - ItemsClientBeginGetSparkJobDefinitionDefinitionOptions contains the optional parameters for the ItemsClient.BeginGetSparkJobDefinitionDefinition
//     method.
func (client *ItemsClient) BeginGetSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, options *ItemsClientBeginGetSparkJobDefinitionDefinitionOptions) (*runtime.Poller[ItemsClientGetSparkJobDefinitionDefinitionResponse], error) {
	return client.beginGetSparkJobDefinitionDefinition(ctx, workspaceID, sparkJobDefinitionID, options)
}

// GetSparkJobDefinitionDefinition - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// When you get a spark job definition's public definition, the sensitivity label is not a part of the definition.
// PERMISSIONS The caller must have read and write permissions for the spark job definition.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// LIMITATIONS This API is blocked for a spark job definition with an encrypted sensitivity label.
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *ItemsClient) getSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, options *ItemsClientBeginGetSparkJobDefinitionDefinitionOptions) (*http.Response, error) {
	var err error
	const operationName = "sparkjobdefinition.ItemsClient.BeginGetSparkJobDefinitionDefinition"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.getSparkJobDefinitionDefinitionCreateRequest(ctx, workspaceID, sparkJobDefinitionID, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// getSparkJobDefinitionDefinitionCreateRequest creates the GetSparkJobDefinitionDefinition request.
func (client *ItemsClient) getSparkJobDefinitionDefinitionCreateRequest(ctx context.Context, workspaceID string, sparkJobDefinitionID string, options *ItemsClientBeginGetSparkJobDefinitionDefinitionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}/getDefinition"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if sparkJobDefinitionID == "" {
		return nil, errors.New("parameter sparkJobDefinitionID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{sparkJobDefinitionId}", url.PathEscape(sparkJobDefinitionID))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	reqQP := req.Raw().URL.Query()
	if options != nil && options.Format != nil {
		reqQP.Set("format", string(*options.Format))
	}
	req.Raw().URL.RawQuery = reqQP.Encode()
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// NewListSparkJobDefinitionsPager - This API supports pagination [/rest/api/fabric/articles/pagination].
// PERMISSIONS The caller must have a viewer workspace role.
// REQUIRED DELEGATED SCOPES Workspace.Read.All or Workspace.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - options - ItemsClientListSparkJobDefinitionsOptions contains the optional parameters for the ItemsClient.NewListSparkJobDefinitionsPager
//     method.
func (client *ItemsClient) NewListSparkJobDefinitionsPager(workspaceID string, options *ItemsClientListSparkJobDefinitionsOptions) *runtime.Pager[ItemsClientListSparkJobDefinitionsResponse] {
	return runtime.NewPager(runtime.PagingHandler[ItemsClientListSparkJobDefinitionsResponse]{
		More: func(page ItemsClientListSparkJobDefinitionsResponse) bool {
			return page.ContinuationURI != nil && len(*page.ContinuationURI) > 0
		},
		Fetcher: func(ctx context.Context, page *ItemsClientListSparkJobDefinitionsResponse) (ItemsClientListSparkJobDefinitionsResponse, error) {
			ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, "sparkjobdefinition.ItemsClient.NewListSparkJobDefinitionsPager")
			nextLink := ""
			if page != nil {
				nextLink = *page.ContinuationURI
			}
			resp, err := runtime.FetcherForNextLink(ctx, client.internal.Pipeline(), nextLink, func(ctx context.Context) (*policy.Request, error) {
				return client.listSparkJobDefinitionsCreateRequest(ctx, workspaceID, options)
			}, nil)
			if err != nil {
				return ItemsClientListSparkJobDefinitionsResponse{}, err
			}
			return client.listSparkJobDefinitionsHandleResponse(resp)
		},
		Tracer: client.internal.Tracer(),
	})
}

// listSparkJobDefinitionsCreateRequest creates the ListSparkJobDefinitions request.
func (client *ItemsClient) listSparkJobDefinitionsCreateRequest(ctx context.Context, workspaceID string, options *ItemsClientListSparkJobDefinitionsOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/sparkJobDefinitions"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	req, err := runtime.NewRequest(ctx, http.MethodGet, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	reqQP := req.Raw().URL.Query()
	if options != nil && options.ContinuationToken != nil {
		reqQP.Set("continuationToken", *options.ContinuationToken)
	}
	req.Raw().URL.RawQuery = reqQP.Encode()
	req.Raw().Header["Accept"] = []string{"application/json"}
	return req, nil
}

// listSparkJobDefinitionsHandleResponse handles the ListSparkJobDefinitions response.
func (client *ItemsClient) listSparkJobDefinitionsHandleResponse(resp *http.Response) (ItemsClientListSparkJobDefinitionsResponse, error) {
	result := ItemsClientListSparkJobDefinitionsResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.SparkJobDefinitions); err != nil {
		return ItemsClientListSparkJobDefinitionsResponse{}, err
	}
	return result, nil
}

// UpdateSparkJobDefinition - PERMISSIONS The caller must have read and write permissions for the spark job definition.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - sparkJobDefinitionID - The spark job definition ID.
//   - updateSparkJobDefinitionRequest - Update spark job definition request payload.
//   - options - ItemsClientUpdateSparkJobDefinitionOptions contains the optional parameters for the ItemsClient.UpdateSparkJobDefinition
//     method.
func (client *ItemsClient) UpdateSparkJobDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, updateSparkJobDefinitionRequest UpdateSparkJobDefinitionRequest, options *ItemsClientUpdateSparkJobDefinitionOptions) (ItemsClientUpdateSparkJobDefinitionResponse, error) {
	var err error
	const operationName = "sparkjobdefinition.ItemsClient.UpdateSparkJobDefinition"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.updateSparkJobDefinitionCreateRequest(ctx, workspaceID, sparkJobDefinitionID, updateSparkJobDefinitionRequest, options)
	if err != nil {
		return ItemsClientUpdateSparkJobDefinitionResponse{}, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return ItemsClientUpdateSparkJobDefinitionResponse{}, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK) {
		err = core.NewResponseError(httpResp)
		return ItemsClientUpdateSparkJobDefinitionResponse{}, err
	}
	resp, err := client.updateSparkJobDefinitionHandleResponse(httpResp)
	return resp, err
}

// updateSparkJobDefinitionCreateRequest creates the UpdateSparkJobDefinition request.
func (client *ItemsClient) updateSparkJobDefinitionCreateRequest(ctx context.Context, workspaceID string, sparkJobDefinitionID string, updateSparkJobDefinitionRequest UpdateSparkJobDefinitionRequest, _ *ItemsClientUpdateSparkJobDefinitionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if sparkJobDefinitionID == "" {
		return nil, errors.New("parameter sparkJobDefinitionID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{sparkJobDefinitionId}", url.PathEscape(sparkJobDefinitionID))
	req, err := runtime.NewRequest(ctx, http.MethodPatch, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, updateSparkJobDefinitionRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// updateSparkJobDefinitionHandleResponse handles the UpdateSparkJobDefinition response.
func (client *ItemsClient) updateSparkJobDefinitionHandleResponse(resp *http.Response) (ItemsClientUpdateSparkJobDefinitionResponse, error) {
	result := ItemsClientUpdateSparkJobDefinitionResponse{}
	if err := runtime.UnmarshalAsJSON(resp, &result.SparkJobDefinition); err != nil {
		return ItemsClientUpdateSparkJobDefinitionResponse{}, err
	}
	return result, nil
}

// BeginUpdateSparkJobDefinitionDefinition - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// To update spark job definition with a public definition, refer to SparkJobDefinitionV1 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v1]
// article for SparkJobDefinitionV1
// format and SparkJobDefinitionV2 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v2] article
// for SparkJobDefinitionV2 format.
// Updating the spark job definition's definition does not affect its sensitivity label.
// PERMISSIONS The caller must have read and write permissions for the spark job definition.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - sparkJobDefinitionID - The spark job definition ID.
//   - updateSparkJobDefinitionRequest - Update spark job definition definition request payload.
//   - options - ItemsClientBeginUpdateSparkJobDefinitionDefinitionOptions contains the optional parameters for the ItemsClient.BeginUpdateSparkJobDefinitionDefinition
//     method.
func (client *ItemsClient) BeginUpdateSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, updateSparkJobDefinitionRequest UpdateSparkJobDefinitionDefinitionRequest, options *ItemsClientBeginUpdateSparkJobDefinitionDefinitionOptions) (*runtime.Poller[ItemsClientUpdateSparkJobDefinitionDefinitionResponse], error) {
	return client.beginUpdateSparkJobDefinitionDefinition(ctx, workspaceID, sparkJobDefinitionID, updateSparkJobDefinitionRequest, options)
}

// UpdateSparkJobDefinitionDefinition - This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
// To update spark job definition with a public definition, refer to SparkJobDefinitionV1 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v1]
// article for SparkJobDefinitionV1
// format and SparkJobDefinitionV2 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v2] article
// for SparkJobDefinitionV2 format.
// Updating the spark job definition's definition does not affect its sensitivity label.
// PERMISSIONS The caller must have read and write permissions for the spark job definition.
// REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support]
// listed in this section.
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object]
// and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
// INTERFACE
// If the operation fails it returns an *core.ResponseError type.
//
// Generated from API version v1
func (client *ItemsClient) updateSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, updateSparkJobDefinitionRequest UpdateSparkJobDefinitionDefinitionRequest, options *ItemsClientBeginUpdateSparkJobDefinitionDefinitionOptions) (*http.Response, error) {
	var err error
	const operationName = "sparkjobdefinition.ItemsClient.BeginUpdateSparkJobDefinitionDefinition"
	ctx = context.WithValue(ctx, runtime.CtxAPINameKey{}, operationName)
	ctx, endSpan := runtime.StartSpan(ctx, operationName, client.internal.Tracer(), nil)
	defer func() { endSpan(err) }()
	req, err := client.updateSparkJobDefinitionDefinitionCreateRequest(ctx, workspaceID, sparkJobDefinitionID, updateSparkJobDefinitionRequest, options)
	if err != nil {
		return nil, err
	}
	httpResp, err := client.internal.Pipeline().Do(req)
	if err != nil {
		return nil, err
	}
	if !runtime.HasStatusCode(httpResp, http.StatusOK, http.StatusAccepted) {
		err = core.NewResponseError(httpResp)
		return nil, err
	}
	return httpResp, nil
}

// updateSparkJobDefinitionDefinitionCreateRequest creates the UpdateSparkJobDefinitionDefinition request.
func (client *ItemsClient) updateSparkJobDefinitionDefinitionCreateRequest(ctx context.Context, workspaceID string, sparkJobDefinitionID string, updateSparkJobDefinitionRequest UpdateSparkJobDefinitionDefinitionRequest, options *ItemsClientBeginUpdateSparkJobDefinitionDefinitionOptions) (*policy.Request, error) {
	urlPath := "/v1/workspaces/{workspaceId}/sparkJobDefinitions/{sparkJobDefinitionId}/updateDefinition"
	if workspaceID == "" {
		return nil, errors.New("parameter workspaceID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{workspaceId}", url.PathEscape(workspaceID))
	if sparkJobDefinitionID == "" {
		return nil, errors.New("parameter sparkJobDefinitionID cannot be empty")
	}
	urlPath = strings.ReplaceAll(urlPath, "{sparkJobDefinitionId}", url.PathEscape(sparkJobDefinitionID))
	req, err := runtime.NewRequest(ctx, http.MethodPost, runtime.JoinPaths(client.endpoint, urlPath))
	if err != nil {
		return nil, err
	}
	reqQP := req.Raw().URL.Query()
	if options != nil && options.UpdateMetadata != nil {
		reqQP.Set("updateMetadata", strconv.FormatBool(*options.UpdateMetadata))
	}
	req.Raw().URL.RawQuery = reqQP.Encode()
	req.Raw().Header["Accept"] = []string{"application/json"}
	if err := runtime.MarshalAsJSON(req, updateSparkJobDefinitionRequest); err != nil {
		return nil, err
	}
	return req, nil
}

// Custom code starts below

// CreateSparkJobDefinition - returns ItemsClientCreateSparkJobDefinitionResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// To create spark job definition with a public definition, refer to SparkJobDefinitionV1 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v1] article for SparkJobDefinitionV1
// format and SparkJobDefinitionV2 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v2] article for SparkJobDefinitionV2 format.
//
// PERMISSIONS The caller must have a contributor workspace role.
//
// # REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
//
// LIMITATIONS
//
//   - To create a spark job definition the workspace must be on a supported Fabric capacity. For more information see: Microsoft Fabric license types
//     [/fabric/enterprise/licenses#microsoft-fabric-license-types].
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - createSparkJobDefinitionRequest - Create item request payload.
//   - options - ItemsClientBeginCreateSparkJobDefinitionOptions contains the optional parameters for the ItemsClient.BeginCreateSparkJobDefinition method.
func (client *ItemsClient) CreateSparkJobDefinition(ctx context.Context, workspaceID string, createSparkJobDefinitionRequest CreateSparkJobDefinitionRequest, options *ItemsClientBeginCreateSparkJobDefinitionOptions) (ItemsClientCreateSparkJobDefinitionResponse, error) {
	result, err := iruntime.NewLRO(client.BeginCreateSparkJobDefinition(ctx, workspaceID, createSparkJobDefinitionRequest, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return ItemsClientCreateSparkJobDefinitionResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return ItemsClientCreateSparkJobDefinitionResponse{}, err
	}
	return result, err
}

// beginCreateSparkJobDefinition creates the createSparkJobDefinition request.
func (client *ItemsClient) beginCreateSparkJobDefinition(ctx context.Context, workspaceID string, createSparkJobDefinitionRequest CreateSparkJobDefinitionRequest, options *ItemsClientBeginCreateSparkJobDefinitionOptions) (*runtime.Poller[ItemsClientCreateSparkJobDefinitionResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.createSparkJobDefinition(ctx, workspaceID, createSparkJobDefinitionRequest, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[ItemsClientCreateSparkJobDefinitionResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[ItemsClientCreateSparkJobDefinitionResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[ItemsClientCreateSparkJobDefinitionResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[ItemsClientCreateSparkJobDefinitionResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// GetSparkJobDefinitionDefinition - returns ItemsClientGetSparkJobDefinitionDefinitionResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// When you get a spark job definition's public definition, the sensitivity label is not a part of the definition.
//
// PERMISSIONS The caller must have read and write permissions for the spark job definition.
//
// # REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
//
// LIMITATIONS This API is blocked for a spark job definition with an encrypted sensitivity label.
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - sparkJobDefinitionID - The spark job definition ID.
//   - options - ItemsClientBeginGetSparkJobDefinitionDefinitionOptions contains the optional parameters for the ItemsClient.BeginGetSparkJobDefinitionDefinition method.
func (client *ItemsClient) GetSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, options *ItemsClientBeginGetSparkJobDefinitionDefinitionOptions) (ItemsClientGetSparkJobDefinitionDefinitionResponse, error) {
	result, err := iruntime.NewLRO(client.BeginGetSparkJobDefinitionDefinition(ctx, workspaceID, sparkJobDefinitionID, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return ItemsClientGetSparkJobDefinitionDefinitionResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return ItemsClientGetSparkJobDefinitionDefinitionResponse{}, err
	}
	return result, err
}

// beginGetSparkJobDefinitionDefinition creates the getSparkJobDefinitionDefinition request.
func (client *ItemsClient) beginGetSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, options *ItemsClientBeginGetSparkJobDefinitionDefinitionOptions) (*runtime.Poller[ItemsClientGetSparkJobDefinitionDefinitionResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.getSparkJobDefinitionDefinition(ctx, workspaceID, sparkJobDefinitionID, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[ItemsClientGetSparkJobDefinitionDefinitionResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[ItemsClientGetSparkJobDefinitionDefinitionResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[ItemsClientGetSparkJobDefinitionDefinitionResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[ItemsClientGetSparkJobDefinitionDefinitionResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// UpdateSparkJobDefinitionDefinition - returns ItemsClientUpdateSparkJobDefinitionDefinitionResponse in sync mode.
// This API supports long running operations (LRO) [/rest/api/fabric/articles/long-running-operation].
//
// To update spark job definition with a public definition, refer to SparkJobDefinitionV1 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v1] article for SparkJobDefinitionV1
// format and SparkJobDefinitionV2 [/rest/api/fabric/articles/item-management/definitions/spark-job-definition-v2] article for SparkJobDefinitionV2 format.
//
// Updating the spark job definition's definition does not affect its sensitivity label.
//
// PERMISSIONS The caller must have read and write permissions for the spark job definition.
//
// # REQUIRED DELEGATED SCOPES SparkJobDefinition.ReadWrite.All or Item.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - sparkJobDefinitionID - The spark job definition ID.
//   - updateSparkJobDefinitionRequest - Update spark job definition definition request payload.
//   - options - ItemsClientBeginUpdateSparkJobDefinitionDefinitionOptions contains the optional parameters for the ItemsClient.BeginUpdateSparkJobDefinitionDefinition method.
func (client *ItemsClient) UpdateSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, updateSparkJobDefinitionRequest UpdateSparkJobDefinitionDefinitionRequest, options *ItemsClientBeginUpdateSparkJobDefinitionDefinitionOptions) (ItemsClientUpdateSparkJobDefinitionDefinitionResponse, error) {
	result, err := iruntime.NewLRO(client.BeginUpdateSparkJobDefinitionDefinition(ctx, workspaceID, sparkJobDefinitionID, updateSparkJobDefinitionRequest, options)).Sync(ctx)
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return ItemsClientUpdateSparkJobDefinitionDefinitionResponse{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return ItemsClientUpdateSparkJobDefinitionDefinitionResponse{}, err
	}
	return result, err
}

// beginUpdateSparkJobDefinitionDefinition creates the updateSparkJobDefinitionDefinition request.
func (client *ItemsClient) beginUpdateSparkJobDefinitionDefinition(ctx context.Context, workspaceID string, sparkJobDefinitionID string, updateSparkJobDefinitionRequest UpdateSparkJobDefinitionDefinitionRequest, options *ItemsClientBeginUpdateSparkJobDefinitionDefinitionOptions) (*runtime.Poller[ItemsClientUpdateSparkJobDefinitionDefinitionResponse], error) {
	if options == nil || options.ResumeToken == "" {
		resp, err := client.updateSparkJobDefinitionDefinition(ctx, workspaceID, sparkJobDefinitionID, updateSparkJobDefinitionRequest, options)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		handler, err := locasync.NewPollerHandler[ItemsClientUpdateSparkJobDefinitionDefinitionResponse](client.internal.Pipeline(), resp, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPoller(resp, client.internal.Pipeline(), &runtime.NewPollerOptions[ItemsClientUpdateSparkJobDefinitionDefinitionResponse]{
			FinalStateVia: runtime.FinalStateViaAzureAsyncOp,
			Handler:       handler,
			Tracer:        client.internal.Tracer(),
		})
	} else {
		handler, err := locasync.NewPollerHandler[ItemsClientUpdateSparkJobDefinitionDefinitionResponse](client.internal.Pipeline(), nil, runtime.FinalStateViaAzureAsyncOp)
		if err != nil {
			var azcoreRespError *azcore.ResponseError
			if errors.As(err, &azcoreRespError) {
				return nil, core.NewResponseError(azcoreRespError.RawResponse)
			}
			return nil, err
		}
		return runtime.NewPollerFromResumeToken(options.ResumeToken, client.internal.Pipeline(), &runtime.NewPollerFromResumeTokenOptions[ItemsClientUpdateSparkJobDefinitionDefinitionResponse]{
			Handler: handler,
			Tracer:  client.internal.Tracer(),
		})
	}
}

// ListSparkJobDefinitions - returns array of SparkJobDefinition from all pages.
// This API supports pagination [/rest/api/fabric/articles/pagination].
//
// PERMISSIONS The caller must have a viewer workspace role.
//
// # REQUIRED DELEGATED SCOPES Workspace.Read.All or Workspace.ReadWrite.All
//
// MICROSOFT ENTRA SUPPORTED IDENTITIES This API supports the Microsoft identities [/rest/api/fabric/articles/identity-support] listed in this section.
//
// | Identity | Support | |-|-| | User | Yes | | Service principal [/entra/identity-platform/app-objects-and-service-principals#service-principal-object] and Managed identities
// [/entra/identity/managed-identities-azure-resources/overview] | Yes |
//
// INTERFACE
// Generated from API version v1
//   - workspaceID - The workspace ID.
//   - options - ItemsClientListSparkJobDefinitionsOptions contains the optional parameters for the ItemsClient.NewListSparkJobDefinitionsPager method.
func (client *ItemsClient) ListSparkJobDefinitions(ctx context.Context, workspaceID string, options *ItemsClientListSparkJobDefinitionsOptions) ([]SparkJobDefinition, error) {
	pager := client.NewListSparkJobDefinitionsPager(workspaceID, options)
	mapper := func(resp ItemsClientListSparkJobDefinitionsResponse) []SparkJobDefinition {
		return resp.Value
	}
	list, err := iruntime.NewPageIterator(ctx, pager, mapper).Get()
	if err != nil {
		var azcoreRespError *azcore.ResponseError
		if errors.As(err, &azcoreRespError) {
			return []SparkJobDefinition{}, core.NewResponseError(azcoreRespError.RawResponse)
		}
		return []SparkJobDefinition{}, err
	}
	return list, nil
}
